<!DOCTYPE html>
<html lang="eng">
    <head>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Playwrite+NG+Modern:wght@100..400&family=Princess+Sofia&family=Sulphur+Point:wght@300;400;700&family=Unkempt:wght@400;700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="../Style/main.css"><!--make sure href has correct pathing--><!--main css for global attributes-->
        <meta charset="UTF-8">
        <meta name="viewport" content="width = device-width, intial-scale=1.0">
        <meta name="author" content="Refilwe">
        <meta name = "description" content = "Basic example of the blog page">
        <title>Essay 2: Digital Literacy and AI</title>
    </head>
    <body>
        <nav class="nav-menu">
            <a href="#" class="menu-hamburger" onclick="toggleNav()">
                <span class="menu-hameburger-line"></span>
                <span class="menu-hameburger-line"></span>
                <span class="menu-hameburger-line"></span>
            </a>
            <ul class="Menu">
                <li><a class="nav-menu-hover" href = "../index.html">Home</a></li>
                <li><a class="nav-menu-hover" href = "BlogHome.html">Blog</a> </li>
                <li><a class="nav-menu-hover" href = "../Projects/Project.html">Portfolio</a></li>
                <li><a class="nav-menu-hover" href = "../Dev Log/DevLogHome.html">Dev Log</a></li>
                <li><a class="nav-menu-hover" href = "../Images/Gallery.html">Gallery</a> </li>
                <li><a class="nav-menu-hover" href = "../Contact/Contact.html">Contact</a></li>
            </ul>
        </nav>
        <div class="layout-center">
            <a href="../Blogs/Blog12.html" class="previous">Previous</a><!--Previous, no next since last-->
            <div class="layout-blog">
            <a href="BlogHome.html" class="layout-blog">Back to Blog Select</a><!--Back to blog select--> 
            <article class = "h-entry">
                <h4 id="Title">Digital Literacy and AI</h4><!--Title-->
                <h5>Refilwe Modise</h5>
                <time datetime="06-12-2025"><h5>June 12</h5></time><!-- Author date and time -->
                <div class="entry">
                    <p>
                        The internet has grown into a hub for everyone to connect, convene, communicate, learn and
                        understand from each other. Social media has been the main driving force this social push with
                        websites such as Facebook, Instagram and X being a few of the major players in the space. The
                        latest endeavour is the integration of Large Language Models (LLM) into these systems to assist
                        users if possible or engage in conversation. But what happens when an LLM spreads
                        misinformation and if possible, can it change public opinion on issues that are important.
                    </p>
                        </br>
                    <p>
                        Analysing the ethics behind LLM systems and how/where its information comes from is
                        important for others to know and allowing advocating for more soft skills towards digital literacy
                        can equip users on their journey in exploring and using the internet. Failure to do so can lead to
                        false information or reactionary sentiment from the public. This was prevalent over the past few
                        weeks as an X employee had allegedly tried to make changes to the LLM system, Grok, to raise
                        awareness for the case of “White Genocide” happening in South Africa. There are also other
                        cases such as Gemini AI misrepresenting diversity of historical figures and the rise of fake news
                        created through AI to distort the facts and provide false realities. Taking these cases into
                        consideration, we can analyse them to view a common thread and provide a reflection of how
                        these incidents could be mitigated.
                    </p>                      
                        </br>
                    <p>
                        Before everything, Grok AI didn’t have any issues as an AI system itself, only around mid-May,
                        did they experience a phenomenon. People started noticing Grok AI providing an unprompted
                        message within Grok’s response. That message was an emphasis on the need to understand
                        that there is a “White Genocide” going on in South Africa, however when people asked further
                        about the details and the need for evidence, Grok would always point to the South African
                        courts that stated that there is no genocide against white people within the country. Grok’s
                        responses also emphasised that the use of the term “Kill the boer” a term said by Economic
                        Freedom Fighter (EFF) leader, Julius Malema, was incitement of genocide towards white famers
                        creating a call to action for them to leave the country for the USA, as they have been granted
                        asylum by the President of the United States of America, Donald Trump.
                    </p>
                        </br>
                        <figure class="essay2-blog-figure-1">
                            <img src="../Blogs/BlogImages/Essay-2-Grok-Post.jpg" alt="Essay 2-Grok Post-Website Figure 1">
                            <figcaption><p><i>Figure 1: Grok AI's response to a user</i></p></figcaption>
                        </figure>
                    <p>
                        Though the employee was not identified many started asking Grok about why it’s doing this. It
                        responded that its “master” was the reason behind its changes and that it was following
                        protocol based on the changes given to them. xAI responded to the situation by publicly
                        publishing Grok’s internal prompts, allowing the users to view the guidelines for how it should
                        respond, allowing users to give feedback when prompt changes need to be adjusted; xAI also
                        reassured users by noting that Grok should be “extremely sceptical” and “not blindly defer to
                        mainstream authority or media.” <i>(Conger, 2025)</i>.
                    </p>
                        </br>
                    <p>
                        The event has also demonstrated digital inequality as this shows users the weaknesses of LLM’s
                        and how vulnerable to change they can be regardless of if the change is internal or external; and
                        with people becoming more reliant on AI in their lives, digital literacy is important. The situation
                        also escalates as AI becomes more advanced for even the human eye to notice any immediate
                        differences and as Raluca Csernatoni mentions, “these tools outpace both governmental
                        oversight and society’s ability to manage the consequences.” <i>(Csernatoni, 2024)</i>.
                        Demonstrating AI’s risks to our communities.
                    </p>
                        </br>
                    <p>
                        However, the silver lining to all this was that the system was able to recognise that it had been
                        fed to provide false information, giving credence to Grok abiding by its core directive of not
                        blindly deferring to mainstream narratives. The hope that more LLM’s are more transparent as to
                        where they get their information from is important, as this allows users to fact check this
                        information and verify the validity of its response. Another good example of this is Google’s,
                        Gemini AI overview; by pressing the hyperlink at certain points in the overview, Gemini, provides
                        supporting information that it made use of, providing transparency and an opportunity for the
                        user to easier catalogue information where necessary.
                    </p>
                        </br>
                        <figure class="essay2-blog-figure-2">
                            <img src="../Blogs/BlogImages/Essay-2-AI-Overview.png" alt="Essay 2-Gemini AI Overview-Website Figure 2">
                            <figcaption><p><i>Figure 2: Screenshot of a Gemini AI Overview</i></p></figcaption>
                        </figure> 
                    <p>
                        Digital literacy is also an important factor to help users navigate the internet as information bias
                        has become more prevalent and widespread among this space. Digital literacy is skills needed
                        to effectively use technology, and skills do so safely and responsible <i>(Lcom Team, 2023)</i>. Digital
                        literacy also allows people, especially children, to strengthen their critical thinking skills as
                        digital skills are becoming more needed in everyday life <i>(Kloza, 2023)</i>. Having these skills can
                        allow users to have more open conversation about the way systems function. A suggestion for
                        the Grok case would be to provide the model specs that Grok uses, to understand the values
                        that Grok has been instilled with prior to processing <i>(Alexander and Kokotajlo, 2025)</i>, that way
                        we have a transparent understanding of the system.
                    </p>
                        </br>
                    <p>
                        Other ways to combat the spread of misinformation is through collaborative initiatives. An
                        article from the world economic forum emphasises the need for collaboration among
                        stakeholders, policymakers, tech companies, researchers and civil organisations to address the
                        challenges posed from the misinformation <i>(Li and Callegari, 2024)</i>. The AI Governance Alliance
                        is one example of a major initiative that focuses on adopting innovation and regulatory
                        approaches to when creating technological and AI systems <i>(World Economic Forum, n.d.)</i>.
                        Therefore, demonstrating one of the many ways that communities are combating
                        misinformation.
                    </p>
                        </br>
                    <p>
                        In conclusion, though the Grok AI case has opened a wound in the reliance of AI systems, there
                        are many ways to equip ourselves from the consequences of misinformation. Digital literacy,
                        collaboration and the need for transparency are important to provide a more safe and secure
                        experience when using the internet. And finally, knowing that AI should be used complementary
                        to the user instead of replacing the user is key in the use of gather information and finding the
                        truth; though AI is advancing much faster, we as a community are becoming more aware of the
                        holes to challenge it.
                    </p>
                </div>
            </article>
            <a href="../Blogs/Essay1.html" class="next">Next</a><!--Next blog, no previous since first-->
            <a href="#Title" class="back-to-top">Back to Top</a> <!-- Back to top of screen-->
        </div>
        </div>
    <footer>
            <p>The night sky with me! I hope you...en...joy</p>
    </footer>
    </body>
</html>